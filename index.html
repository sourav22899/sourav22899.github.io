<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- original template from from url=(0035)http://www.cs.berkeley.edu/~barron/ -->
<html lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
     <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
	
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link rel="icon" type="image/png" href="http://www.cs.berkeley.edu/~barron/seal_icon.png">
	
    <title>Sourav Sahoo</title>
    
    <link href="/img/css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
      <tbody><tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center">
                  <name>Sourav Sahoo</name>
                  
                </p><p align="">I am a fourth-year student in the dual degree program at the <a href="http://www.ee.iitm.ac.in/"> Department of
                  Electrical Engineering at Indian Institute of Technology Madras, Chennai</a>. My primary research interests are machine
                  learning theory, deep learning, and optimization.
                  <br>
<!--                  <strong>I am currently looking for exciting research internships in these domains for the summer of 2021.</strong>-->

                </p><p align="center">
			
                <a href="mailto:sourav.sahoo@smail.iitm.ac.in">Email</a> &nbsp;/&nbsp
                <a href="https://scholar.google.co.in/citations?hl=en&authuser=1&user=IWZJF8wAAAAJ">Google Scholar</a> /&nbsp
                <a href="https://www.researchgate.net/profile/Sourav_Sahoo4">ResearchGate</a> /&nbsp
                <a href="https://www.linkedin.com/in/sourav-sahoo-68a07615a/"> LinkedIn </a> /&nbsp
                <a href="https://twitter.com/sourav22899"> Twitter </a> /&nbsp
                <a href="https://github.com/sourav22899"> GitHub </a> /&nbsp
                <a href="./files/cv_updated.pdf"> CV </a>
                </p>
              </td>
			  <td width="33%">
			  <img src="./img/20190510_111651.jpg" alt="" width="100%" align="top">
			  </td>
<!-- 				<td> <img src="./img/20190510_111651.jpg" style="width: 200;"></td></tr>  -->
          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>News</heading>
                  <p> [2021.06] Our <a href="https://arxiv.org/abs/2012.00096v2">paper</a> was accepted at <a href="https://biokdd.org/biokdd21/index.html">BIOKDD 2021</a>, Virtual.</p>
                  <p> [2020.08] Attended the <a href="https://sites.google.com/view/aisummerschool2020/home?authuser=0">
            Google Research India AI Summer School, 2020</a>.</p>
                  <p> [2019.09] Our <a href="./files/segment_ser_paper64.pdf">paper</a> was accepted at <a href="https://www.acpr2019.org/">ACPR 2019</a>, Auckland, New Zealand.</p>
              </td>
            </tr>
          </tbody></table>

	  <!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Publications and Preprints</heading>
              </td>
            </tr>
          </tbody></table>
	  
	
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>
               <!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/vis_rep.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://link.springer.com/chapter/10.1007/978-3-030-41299-9_34">
                        <papertitle>A Segment Level Approach to Speech Emotion Recognition using Transfer Learning</papertitle></a>
                        <br><strong>Sourav Sahoo</strong>, Puneet Kumar, Balasubramanian Raman and Partha Pratim Roy<br>
                        <strong>Asian Conference on Pattern Recognition (ACPR 2019), Auckland, New Zealand, November 2019</strong> <br>
                <a href="./files/segment_ser_paper64.pdf">Paper</a> /
                        <a href="./files/segment_ser_supplement_paper64.pdf">Supplementary</a> /
                        <a href="./files/acpr_paper_64_poster.pdf">Poster</a> /
                        <a href="https://github.com/sourav22899/segment-ser">Code</a> /
                        <a href="./files/acpr_paper.bib">BibTex</a>
               <p></p>
               <p>In this paper, we propose a speech emotion recognition system that predicts emotions for multiple segments of a single audio clip unlike the conventional emotion recognition models that predict the emotion of an entire audio clip directly. The proposed system consists of a pre-trained deep convolutional neural network (CNN) followed by a single layered neural network which predicts the emotion classes of the audio segments. The predictions for the individual segments are finally combined to predict the emotion of a particular clip. The proposed model attains an accuracy of 68.7% surpassing the current state-of-the-art models in classifying the data into one of the four emotional classes <i>(angry, happy, sad and neutral)</i> when trained and evaluated on IEMOCAP audio-only dataset.

		</p><p></p><p></p>
              </td>
            </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>
               <td width="25%"><img src="./img/full_system1.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	        <a href="https://arxiv.org/abs/2012.00096">
                <papertitle>Multi-Modal Detection of Alzheimer's Disease from Speech and Text</papertitle></a>
               <br>Amish Mittal*, <strong>Sourav Sahoo</strong>*, Arnhav Datar*, Juned Kadiwala*, Hrithwik Shalu and Jimson Mathew<br>
               <strong>(Preprint)</strong><br>
               <a href="https://arxiv.org/abs/2012.00096">ArXiv Version</a> /
               <a href="./files/alzheimers.bib">BibTex</a>
                <p></p>
               <p>Reliable detection of the prodromal stages of Alzheimer's disease (AD) remains difficult even today because, unlike other neurocognitive impairments, there is no definitive diagnosis of AD <i>in vivo</i>. In this work, we propose a multimodal deep learning method that utilizes speech and the corresponding transcript simultaneously to detect AD. We also perform experiments to analyze the model performance when Automated Speech Recognition (ASR) system generated transcripts are used instead of manual transcription in the text-based model. The proposed method achieves 85.3% 10-fold cross-validation accuracy when trained and evaluated on the DementiaBank Pitt corpus.
                   <br><br><small>*Authors contributed equally</small><br>
		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
<!--	              <td width="25%"><img src="./img/vis_rep.png" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>Planar Geometry and Latest Scene Recovery from a Single Motion Blurred Image</papertitle></a><br><strong>Kuldeep Purohit</strong>, Subeesh Vasu, M. Purnachandra Rao, and A.N. Rajagopalan<br>-->
<!--                  <strong>Under Review</strong>* <br>-->
<!--                  <a href="https://arxiv.org/abs/1904.03710">ArXiv Version</a> -->
<!--           &lt;!&ndash;       <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p>Existing works on motion deblurring either ignore the effects of depth-dependent blur or work with the assumption of a -->
<!--			multi-layered scene wherein each layer is modeled in the form of fronto-parallel plane. In this work, we consider-->
<!--			the case of 3D scenes with piecewise planar structure i.e., a scene that can be modeled as a combination of multiple -->
<!--			planes with arbitrary orientations. We first propose an approach for estimation of normal of a planar scene from a -->
<!--			single motion blurred observation. We then develop an algorithm for automatic recovery of a number of planes, the -->
<!--			parameters corresponding to each plane, and camera motion from a single motion blurred image of a multiplanar 3D scene.-->
<!--			Finally, we propose a first-of-its-kind approach to recover the planar geometry and latent image of the scene by -->
<!--			adopting an alternating minimization framework built on our findings. Experiments on synthetic and real data reveal -->
<!--			that our proposed method achieves state-of-the-art results.-->
<!--                    &lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->


<!--           <td width="25%"><img src="./img/UW_Dehazing.png" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>Multi-level Weighted Enhancment for Underwater Image Dehazing</papertitle></a><br><strong>Kuldeep Purohit</strong>, Srimanta Mandal and A.N. Rajagopalan<br>-->
<!--                  <strong>Journal of the Optical Society of America A (JOSA-A) </strong>* <br>-->
<!--                  <a href="https://www.osapublishing.org/josaa/abstract.cfm?uri=josaa-36-6-1098">Paper Link</a> -->
<!--           &lt;!&ndash;       <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p>Attenuation and scattering of light are responsible for haziness in images of underwater-->
<!--scenes. We propose an approach to reduce this effect, based on the underlying principle is that enhancement at different-->
<!--levels of detail can undo the degradation caused by underwater haze. The depth information is-->
<!--captured implicitly while going through different levels of details due to depth-variant nature of-->
<!--haze. Hence, we judiciously assign weights to different levels of image details and reveal that-->
<!--their linear combination along with the coarsest information can successfully restore the image.-->
<!--Results demonstrate the efficacy of our approach as compared to state-of-the-art underwater-->
<!--dehazing methods. -->
<!--</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--&lt;!&ndash;Next paper should be listed in bottom &ndash;&gt;-->
<!--	              <td width="25%"><img src="./files/ICCV_thumbprint.png" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>EFFICIENT MOTION DEBLURRING WITH FEATURE TRANSFORMATION AND SPATIAL ATTENTION</papertitle></a><br><strong>Kuldeep Purohit</strong> and A.N. Rajagopalan<br>-->
<!--                  <strong>Accepted at the IEEE International Conference on Image Processing (ICIP) 2019</strong>* <br>-->
<!--                  <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> -->
<!--           &lt;!&ndash;       <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> A preliminary version of our work "Spatially-Adaptive Residual Networks for Efficient Image and Video Deblurring"</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--		<tbody>-->
<!--		   <tr>-->


<!--&lt;!&ndash;Next paper should be listed in bottom &ndash;&gt;-->
<!--	              <td width="25%"><img src="./img/PD_plot.png" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	        &lt;!&ndash;        <p><a href="https://arxiv.org/abs/1605.07651"> &ndash;&gt;-->
<!--	<papertitle>Scale-Recurrent Multi-residual Dense Network for Image Super-Resolution</papertitle></a><br><strong>Kuldeep Purohit</strong>*, Srimanta Mandal, and A.N. Rajagopalan<br>-->
<!--			                  <strong>PIRM Workshop and Challenge, Eurpean Conference on Computer Vision Workshops (ECCVW 2018), Munich, Germany, September 2018</strong> <br>-->

<!-- <a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Purohit_Scale-Recurrent_Multi-Residual_Dense_Network_for_Image_Super-Resolution_ECCVW_2018_paper.pdf">Paper</a> /-->
<!-- <a href="./files/Poster_ECCV2018.pdf">Poster</a> -->

<!--          &lt;!&ndash;       <a href="https://arxiv.org/abs/1804.02913.pdf">ArXiv Pre-print</a>-->
<!--	  	  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a> /-->
<!--                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/SanginetoNCS16.bib">bibtex</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p>A preliminary version of our Neurocomputing work, presented at the European Conference on Computer Vision (ECCV) - Perceptual Image Restoration and Manipulation (PIRM) Workshop 2018. Our team REC-SR was a <strong>finalist in all three regions of the Super Resolution Challenge</strong> (https://www.pirm2018.org/PIRM-SR.html).-->
<!--</br></br> -->
<!--&lt;!&ndash; <small>*Authors contributed equally</small>&ndash;&gt;-->

<!--		</p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->




<!--&lt;!&ndash;Next paper should be listed in bottom &ndash;&gt;-->
<!--	              <td width="25%"><img src="./img/blur_detection.jpg" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	   &lt;!&ndash;              <p><a href="https://arxiv.org/abs/1610.00307"> &ndash;&gt;-->
<!--	<papertitle>Learning based Blur Detection and Segmentation</papertitle></a><br>-->
<!--		 <strong>Kuldeep Purohit</strong>, Anshul B. Shah, and A.N. Rajagopalan<br>-->
<!--                  <strong>IEEE International Conference on Image Processing (ICIP 2018), Athens, Greece, October 2018  </strong> <br>-->
<!--                  <a href="https://ieeexplore.ieee.org/document/8451765">Paper Link</a> /-->
<!--                  <a href="./files/ICIP2018_supplementary.pdf">Supplementary</a> /-->
<!--                  <a href="./files/POSTER_ICIP2018.pdf">Poster</a> -->
<!--        &lt;!&ndash;           <em>arXiv:1610.00307</em>, 2016 &nbsp; <br>-->
<!--                  <a href="https://arxiv.org/pdf/1610.00307v1.pdf">PDF</a> /-->
<!--                  <a href="#">bibtex</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p>We present a robust two-level architecture for blur-based segmentation of a single image. First network is a fully convolutional encoder-decoder for estimating a semantically meaningful blur map from the full-resolution blurred image. Second network is a CNN-based classifier for obtaining local (patch-level) blur-probabilities. Fusion of the two network outputs enables accurate blur-segmentation using Graph-cut optimization over the obtained probabilities. We also show its applications in blur magnification and matting.-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->


<!--&lt;!&ndash;Next paper should be listed in bottom &ndash;&gt;-->
<!--	              <td width="25%"><img src="./img/icvgip2019.png" alt="PontTuset" width="150" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	   &lt;!&ndash;              <p><a href="https://arxiv.org/abs/1610.00307"> &ndash;&gt;-->
<!--	<papertitle>Color Image Super Resolution in Real Noise</papertitle></a><br>-->
<!--		 Srimanta Mandal, <strong>Kuldeep Purohit</strong>, and A.N. Rajagopalan<br>-->
<!--                  <strong>Accepted for Oral Presentation at ACM Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP 2018), IIIT Hyderabad, India, december 2018 </strong> <br>-->
<!--                   <a href="./files/ICVGIP2018_Cam.pdf">Accepted Version</a> -->
<!--         &lt;!&ndash;         <em>arXiv:1610.00307</em>, 2016 &nbsp; <br>-->
<!--                  <a href="https://arxiv.org/pdf/1610.00307v1.pdf">PDF</a> /-->
<!--                  <a href="#">bibtex</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> Proposed an approach to super-resolve noisy color images by considering the color channels-->
<!--jointly. Implicit low-rank structure of visual data is enforced via nuclear norm minimization-->
<!--in association with color channel-dependent weights, which are added as a regularization-->
<!--term to the cost function. Additionally, multi-scale details of the image are added to the-->
<!--model through another regularization term that involves projection onto PCA basis, which-->
<!--is constructed using similar patches extracted across different scales of the input image. <strong> Selected for the Best Paper Award (Runner Up) </strong>*: https://cvit.iiit.ac.in/icvgip18/bestpaperaward.php.-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->


<!--&lt;!&ndash;Next paper should be listed in bottom &ndash;&gt;-->
<!--	              <td width="25%"><img src="./img/ICVGIP.png" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>Mosaicing Deep Underwater Imagery</papertitle></a><br><strong>Kuldeep Purohit</strong>,Subeesh Vasu, A.N. Rajagopalan, V Bala Naga Jyothi, and Ramesh Raju<br>-->
<!--                  <strong>ACM Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP 2016), IIT Guwahati, India, december 2016 </strong> &nbsp; <br>-->
<!--                  <a href="http://www.ee.iitm.ac.in/~ee13d050/pdf/2016_Kuldeep_Deep_icvgip.pdf">main paper</a> /-->
<!--		<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">Supplementary</a> /-->
<!--		<a href="./files/POSTER_ICVGIP2016.pdf">Poster</a> -->
<!--           &lt;!&ndash;       <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a>  &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p>This work deals with the problem of mosaicing deep underwater images (captured by Remotely Operated Vehicles), which suffer from haze, color-cast, and non-uniform illumination. We propose a framework that restores these images in accordance with a suitably derived degradation model. Furthermore, our scheme harnesses the scene-depth information present in the haze for non-rigid registration of the images before blending to construct a mosaic that is free from artifacts such as local blurring, ghosting, and visible seams.-->
<!--</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--&lt;!&ndash;Next paper should be listed in bottom &ndash;&gt;-->
<!--	              <td width="25%"><img src="./img/splicing.png" alt="PontTuset" width="170" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>Splicing Localization in Motion Blurred 3D scenes</papertitle></a><br><strong>Kuldeep Purohit</strong> and A.N. Rajagopalan<br>-->
<!--                  <strong>IEEE International Conference on Image Processing (ICIP 2016), Phoenix, Arizona, September 2016</strong> &nbsp; <br>-->
<!--                  <a href="https://ieeexplore.ieee.org/abstract/document/7533095">Paper</a> /				-->
<!--                  <a href="./files/ICIP2016_supplementary.pdf">Supplementary</a> /-->
<!--                  <a href="./files/POSTER_ICIP2016.pdf">Poster</a> -->
<!--      &lt;!&ndash;            <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> This work proposes an efficient algorithm for depth based segmentation using spatially-distributed blur-kernels present in a single motion-blurred image of a 3D scene. The segmentation is then further utilized to estimate global camera motion from a single blurred image of a 3D scene. Finally, local blur profiles are compared with the global motion model to highlight inconsistencies and detect spliced regions. -->
<!--</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--            <tbody><tr>-->
<!--              <td>-->
<!--                <heading>Co-authored Workshop Proceedings</heading>-->
<!--              </td>-->
<!--            </tr>-->
<!--          </tbody></table>-->
<!--           -->
<!--          <table width="100%" align="center" border="0" cellpadding="20">-->
<!--            <tbody><tr>-->

<!--	              <td width="25%"><img src="./files/NTIRE2019.jpeg" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>NTIRE 2019 Challenge on Image Colorization: Report</papertitle></a><br>-->
<!--Shuhang Gu, Radu Timofte, Richard Zhang, Maitreya Suin, <strong>Kuldeep Purohit</strong> , A. N. Rajagopalan, Athi Narayanan S., Jameer Babu Pinjari, Zhiwei Xiong, Zhan Shi, Chang Chen, Dong Liu, Manoj Sharma, Megh Makwana, Anuj Badhwar, Ajay Pratap Singh, Avinash Upadhyay, Akkshita Trivedi, Anil Saini, Santanu Chaudhury, Prasen Kumar Sharma, Priyankar Jain, Arijit Sur, Gokhan Ozbulak <br>-->
<!--                  <strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</strong> <br>-->
<!--                  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Gu_NTIRE_2019_Challenge_on_Image_Colorization_Report_CVPRW_2019_paper.pdf">PDF (Openacess)</a> -->
<!--          &lt;!&ndash;        <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> Report describing various solutions to the Challenge on Image Colorization, NTIRE 2019</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--		<tbody>-->
<!--		   <tr>		   -->
<!--			   <td width="25%"><img src="./files/NTIRE2019.jpeg" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>NTIRE 2019 Challenge on Video Deblurring: Methods and Results</papertitle></a><br>-->
<!--Nah, Seungjun and Timofte, Radu and Baik, Sungyong and Hong, Seokil and Moon, Gyeongsik and Son, Sanghyun and Lee, Kyoung Mu and Wang, Xintao and Chan, Kelvin C.K. and Yu, Ke and Dong, Chao and Loy, Chen Change and Fan, Yuchen and Yu, Jiahui and Liu, Ding and Huang, Thomas S. and Sim, Hyeonjun and Kim, Munchurl and Park, Dongwon and Kim, Jisoo and Chun, Se Young and Haris, Muhammad and Shakhnarovich, Greg and Ukita, Norimichi and Zamir, Syed Waqas and Arora, Aditya and Khan, Salman and Khan, Fahad Shahbaz and Shao, Ling and Gupta, Rahul Kumar and Chudasama, Vishal and Patel, Heena and Upla, Kishor and Fan, Hongfei and Li, Guo and Zhang, Yumei and Li, Xiang and Zhang, Wenjie and He, Qingwen and <strong>Purohit, Kuldeep</strong> and Rajagopalan, A. N. and Kim, Jeonghun and Tofighi, Mohammad and Guo, Tiantong and Monga, Vishal <br>-->
<!--                  <strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</strong> <br>-->
<!--                  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Deblurring_Methods_and_Results_CVPRW_2019_paper.pdf">PDF (Openacess)</a> -->
<!--          &lt;!&ndash;        <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> -->
<!--                  <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> Report describing various solutions to the Challenge on Video Deblurring, NTIRE 2019</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--		<tbody>-->
<!--		   <tr>-->
<!--			   <td width="25%"><img src="./files/NTIRE2019.jpeg" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>NTIRE 2019 Challenge on Video Super-Resolution: Methods and Results</papertitle></a><br>-->
<!--Nah, Seungjun and Timofte, Radu and Gu, Shuhang and Baik, Sungyong and Hong, Seokil and Moon, Gyeongsik and Son, Sanghyun and Lee, Kyoung Mu and Wang, Xintao and Chan, Kelvin C.K. and Yu, Ke and Dong, Chao and Loy, Chen Change and Fan, Yuchen and Yu, Jiahui and Liu, Ding and Huang, Thomas S. and Liu, Xiao and Li, Chao and He, Dongliang and Ding, Yukang and Wen, Shilei and Porikli, Fatih and Kalarot, Ratheesh and Haris, Muhammad and Shakhnarovich, Greg and Ukita, Norimichi and Yi, Peng and Wang, Zhongyuan and Jiang, Kui and Jiang, Junjun and Ma, Jiayi and Dong, Hang and Zhang, Xinyi and Hu, Zhe and Kim, Kwanyoung and Kang, Dong Un and Chun, Se Young and <strong>Purohit, Kuldeep</strong> and Rajagopalan, A. N. and Tian, Yapeng  and Zhang, Yulun and Fu, Yun and Xu, Chenliang and Tekalp, A. Murat and Yilmaz, M. Akin and Korkmaz, Cansu and Sharma, Manoj and Makwana, Megh and Badhwar, Anuj and Singh, Ajay Pratap and Upadhyay, Avinash and Mukhopadhyay, Rudrabha and Shukla, Ankit and Khanna, Dheeraj and Mandal, A.S. and Chaudhury, Santanu and Miao, Si and Zhu, Yongxin and Huo, Xiao <br>-->
<!--                  <strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</strong> <br>-->
<!--                  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Super-Resolution_Methods_and_Results_CVPRW_2019_paper.pdf">PDF (Openacess)</a> -->
<!--          &lt;!&ndash;        <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> -->
<!--                  <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> Report describing various solutions to the Challenge on Video Super-Resolution, NTIRE 2019</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--		<tbody>-->
<!--		   <tr>-->
<!--               <td width="25%"><img src="./files/NTIRE2019.jpeg" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>NTIRE 2019 Image Dehazing Challenge Report</papertitle></a><br>-->
<!--Codruta O. Ancuti, Cosmin Ancuti, Radu Timofte, Luc Van Gool, Lei Zhang,Ming-Hsuan Yang, Tiantong Guo, Xuelu Li, Venkateswararao Cherukuri, Vishal Monga, Hao Jiang, Siyuan Yang, Yan Liu, Xiaochao Qu, Pengfei Wan, Dongwon Park, Se Young Chun, Ming Hong, Jinying Huang, Yizi Chen, Shuxin Chen, Bomin Wang, Pablo Navarrete Michelini, Hanwen Liu, Dan Zhu, Jing Liu, Sanchayan Santra, Ranjan Mondal , Bhabatosh Chanda, Peter Morales, Tzofi Klinghoffer, Le Manh Quan, Yong-Guk Kim, Xiao Liang, Runde Li, Jinshan Pan, Jinhui Tang, <strong>Kuldeep Purohit</strong> , Maitreya Suin, Raimondo Schettini, Simone Bianco, Flavio Piccoli, C. Cusano, Luigi Celona, Sunhee Hwang, Hyeran Byun, Subrahmanyam Murala, Akshay Dudhane, Harsh Aulakh, Zheng Tianxiang, Tao Zhang, Weining Qin, Runnan Zhou, Shanhu Wang, Jean-Philippe Tarel, Chuansheng Wang, Jiawei Wu <br>-->
<!--                  <strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</strong> <br>-->
<!--                  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Ancuti_NTIRE_2019_Image_Dehazing_Challenge_Report_CVPRW_2019_paper.pdf">PDF (Openacess)</a> -->
<!--          &lt;!&ndash;        <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> -->
<!--                  <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> Report describing various solutions to the Challenge on Image Dehazing, NTIRE 2019</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--		<tbody>-->
<!--		   <tr>-->


<!--SECTION -->




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Other Projects</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody><tr>
                <td width="25%"><img src="./img/01_noise_hedge.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>An Empirical Study on Online Agnostic Boosting via Regret Minimization</papertitle></a>
                        <br><strong>Sourav Sahoo</strong><br>
                        <strong>EE6180 Final Project</strong><br>
                        <a href="./files/ee6180_project_report.pdf">Report</a> /
                        <a href="./files/ee6180_project_slides.pdf">Slides</a> /
                        <a href="https://youtu.be/YeNmxHxQg2M">Video</a> /
                        <a href="https://github.com/sourav22899/ee6180-theoretical-ml/tree/master/ee6180_project">Code</a>
                </p><p></p>
                <p> Efficient boosting algorithms have existed for both realizable and agnostic settings in the statistical learning framework, but only for the realizable case in the online setting. In the <a href="https://papers.nips.cc/paper/2020/hash/07168af6cb0ef9f78dae15739dd73255-Abstract.html">recent paper</a>, Brukhim <i>et al.</i> proposed the first agnostic online boosting algorithm that achieved sublinear regret. The algorithm efficiently converts an arbitrary online convex optimizer into an online booster. This algorithm also extends to statistical as well as online realizable setting. In this work, we briefly state the main results of the above-mentioned paper and conduct experiments on two different datasets to study the proposed algorithm's empirical performance.
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>


	              <td width="25%"><img src="./img/2_params.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Convergence and Implicit Regularization of Stochastic Mirror Descent in Overparameterized Models</papertitle></a>
                        <br><strong>Sourav Sahoo</strong><br>
                        <strong>EE5121 Term Paper</strong><br>
                        <a href="./files/ee_5121_term_paper.pdf">Report</a> /
                        <a href="https://github.com/sourav22899/ee5121-convex/tree/master/term-paper">Code</a>
                </p><p></p>
                <p> Stochastic Mirror Descent (SMD) algorithms comprise a family of algorithms that are increasingly being used in multiple domains such as machine learning, optimization, signal processing, and control. Like Stochastic Gradient Descent (SGD), SMD algorithms perform updates along the negative gradient of a stochastically chosen loss function. However, instead of performing updates directly on the objective parameter, updates are performed in a "mirror domain" whose transformation is given by the gradient of a strictly convex function. In this work, we explicitly shed some light on <i>convergence</i> and <i>implicit regularization</i> due to stochastic mirror descent in overparameterized linear and non-linear models.
</br></br>
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--	              <td width="25%"><img src="./files/segment_ser_supplement_paper64.pdf" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>Multiple Target Detection and Tracking in Wide Area Surveillance</papertitle></a><br><strong>Kuldeep Purohit</strong> and Arshad Jamal (Scientist E)<br>-->
<!--                  <strong>Project under Centre for Artificial Intelligence and Robotics, Defense Research and Development Organization, India</strong> &nbsp; <br>-->
<!--                  2012 &nbsp; <br>-->
<!--                 <a href="./files/segment_ser_supplement_paper64.pdf">Report</a>-->
<!--	&lt;!&ndash; 	<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">supplementary</a> / &ndash;&gt;-->
<!--           &lt;!&ndash;       <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a>  &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> In this project, the problem of object detection and tracking in the challenging domain of wide area surveillance-->
<!--			has been tackled. This problem poses several challenges: large camera motion, strong parallax, large number of moving-->
<!--			objects, and small number of pixels on target, single channel data and low frame-rate of video. The method implemented-->
<!--			here overcomes these challenges when tested on UAV videos.-->
<!--</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--Next paper should be listed in bottom -->
<!--

	              <td width="25%"><img src="./img/PhDthesis.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://arxiv.org/abs/1512.07314">
	<papertitle>Mid-level Representation for Visual Recognition</papertitle></a><br>
                  <strong>Moin Nabi</strong><br>
                  <em>Ph.D. Dissertation </em>, 2015 &nbsp; <br>
                  <a href="./files/PhD_thesis.pdf">PDF</a> /
                  <a href="./files/thesis_slide.pdf">slides</a> /
                  <a href="https://www.youtube.com/watch?v=6IY-0swKaiM">talk</a> /
                </p><p></p>
                <p>This thesis targets employing mid-level representations for different high-level visual
recognition tasks, both in image and video understanding.
                </p><p></p>
                <p></p>
              </td>
            </tr>

-->

<!--Next paper should be listed in bottom -->

<!--
	              <td width="25%"><img src="./img/stock.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Stock trend prediction using Twin Gaussian Process regression</papertitle></a><br>M. Mojaddady, <strong>M. Nabi</strong>, S. Khadivi<br>
                  <em>Technical Report</em>, 2011 &nbsp;<br>
                  <a href="./files/stock.pdf">PDF</a> /
                  <a href="./files/stock.bib">bibtex</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>

-->

<!--Next paper should be listed in bottom -->

<!--

	              <td width="25%"><img src="./img/matlab3.jpg" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>A Turorial on Digital Image Processing using MATLAB</papertitle></a><br><strong>M. Nabi</strong><br>
                  <em>National Digital Image Processing Workshop</em>, 2008 &nbsp;<br>
                  <a href="./files/tutorial.zip">PDF</a> /
		  <a href="./files/code.zip">code</a> /
		  <a href="./files/images.tar.gz">data</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>

-->

<!--                <br>
                <p align="right"><font size="3">
			Erd&ouml;s = 3 (via two paths)
                  </font>
                <br>
-->


 <!-- 		<p align="right"><font size="2">
                  <a href="http://www.cs.berkeley.edu/~barron/">Thanks Jon Barron</a>
                  </font>
                </p>
              </td>
            </tr>
          </tbody></table>
          <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
                    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

          </script><script src="./img/ga.js" type="text/javascript"></script> <script type="text/javascript">
            try {
                    var pageTracker = _gat._getTracker("UA-7580334-1");
                    pageTracker._trackPageview();
                    } catch(err) {}

          </script>
        </td>
      </tr>
    </tbody></table>
 -->

	            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
<div style="text-align: left;"><script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5fz2vvb1pjx&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
              </td>
            </tr>
          </tbody></table>



	<!-- <script type="text/javascript" src="//ri.revolvermaps.com/0/0/1.js?i=834fq7qvtyr&amp;s=182&amp;m=0&amp;v=false&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000"align="left" async="async"></script></div>
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5fz2vvb1pjx&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script> -->
</body></html>
