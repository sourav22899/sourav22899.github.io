<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- original template from from url=(0035)http://www.cs.berkeley.edu/~barron/ -->
<link href='https://fonts.googleapis.com/css?family=Inconsolata' rel='stylesheet'>
<html lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
     <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Inconsolata', "Gill Sans", "Gill Sans MT", "Optima", 'Helvetica', Lato, Verdana, sans-serif;
    font-size: 16px
    }
    strong {
    font-family: 'Inconsolata', "Gill Sans", "Gill Sans MT", "Optima", 'Helvetica', Lato, Verdana, sans-serif;
    font-size: 16px;
    }
    heading {
    font-family: 'Inconsolata', "Gill Sans", "Gill Sans MT", "Optima", 'Helvetica', Lato, Verdana, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Inconsolata', "Gill Sans", "Gill Sans MT", "Optima", 'Helvetica', Lato, Verdana, sans-serif;
    font-size: 16px;
    }
    name {
    font-family: 'Inconsolata', "Gill Sans", "Gill Sans MT", "Optima", 'Helvetica', Lato, Verdana , sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
	
    <link href='http://fonts.googleapis.com/css?family=Helvetica:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link rel="icon" type="image/png" href="http://www.cs.berkeley.edu/~barron/seal_icon.png">
	
    <title>Sourav Sahoo</title>
    
    <link href="/img/css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
      <tbody><tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center">
                  <name>Sourav Sahoo</name>
                  
                </p><p align="">I am a first-year PhD student at the <a href="https://orc.mit.edu/">Operations Research Center</a> at <a href="https://mit.edu/">MIT</a> advised by <a href="https://www.mit.edu/~golrezae/">Prof. Negin Golrezaei.</a> I graduated with a dual degree (B.Tech + M.Tech) from the Department of
                  <a href="http://www.ee.iitm.ac.in/">Electrical Engineering</a> at the <a href="https://www.iitm.ac.in/">Indian Institute of Technology Madras, Chennai</a> where I was fortunate to be advised by <a href="https://www.tifr.res.in/~abhishek.sinha/">Prof. Abhishek Sinha</a>. My primary research interests are optimization, mechanism design, auctions, game theory and online learning.
                  <br>

                </p><p align="center">
			
                <a href="mailto:sahoo.sourav1999@gmail.com">Email</a> /&nbsp
                <a href="https://scholar.google.co.in/citations?hl=en&authuser=1&user=IWZJF8wAAAAJ">Google Scholar</a> /&nbsp
<!--                <a href="https://www.researchgate.net/profile/Sourav_Sahoo4">ResearchGate</a> /&nbsp-->
                <a href="https://www.linkedin.com/in/sourav-sahoo-68a07615a/"> LinkedIn </a> /&nbsp
                <a href="https://twitter.com/sourav22899"> Twitter </a> /&nbsp
                <a href="https://github.com/sourav22899"> GitHub </a> /&nbsp
                <a href="./files/cv_updated.pdf"> CV </a>
                </p>
              </td>
			  <td width="33%">
			  <img src="./img/20190510_111651.jpg" alt="" width="100%" align="top">
			  </td>
<!-- 				<td> <img src="./img/20190510_111651.jpg" style="width: 200;"></td></tr>  -->
          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>News</heading>
                  <p> [2024.06] Our <a href="https://arxiv.org/abs/2406.03674">paper</a> on uniform price auctions is out.</p>
                  <p> [2023.09] Started PhD at MIT!</p>
                  <p> [2022.09] Our <a href="https://arxiv.org/abs/2209.14222">paper</a> on online subset selection is out.</p>
                  <p> [2022.07] Graduated from IIT Madras!</p>
                  <p> [2022.02] Our <a href="https://arxiv.org/abs/2109.12340">paper</a> on distributed online optimization was accepted at <a href="https://acc2022.a2c2.org/">ACC 2022.</a></p>
<!--                  <p> [2022.01] Our <a href="https://arxiv.org/abs/2110.07881">paper</a> on k-experts problem was accepted at <a href="https://aistats.org/aistats2022/">AISTATS 2022.</a></p>-->
<!--                  <p> [2021.06] Our <a href="https://arxiv.org/abs/2012.00096v2">paper</a> was accepted at <a href="https://biokdd.org/biokdd21/index.html">BIOKDD 2021.</a></p>-->
<!--                  <p> [2020.08] Attended the <a href="https://sites.google.com/view/aisummerschool2020/home?authuser=0">-->
<!--            Google Research India AI Summer School, 2020</a>.</p>-->
<!--                  <p> [2019.09] Our <a href="./files/segment_ser_paper64.pdf">paper</a> was accepted at <a href="https://www.acpr2019.org/">ACPR 2019.</a></p>-->
              </td>
            </tr>
          </tbody></table>

	  <!--SECTION -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Preprints</heading>
              </td>
            </tr>
          </tbody></table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>
<!--               <td width="25%"><img src="./img/n=2498_alpha=0.1.svg" alt="PontTuset" width="200" style="border-style: none">-->
	              </td><td width="75%" valign="top">
	        <a href="https://arxiv.org/abs/2406.03674">
                <papertitle>Bidding in Uniform Price Auction for Value Maximizing Buyers</papertitle></a>
               <br>Negin Golrezaei and <span style="text-decoration: underline;">Sourav Sahoo</span><br>
<!--               International Conference on Artificial Intelligence and Statistics (AISTATS), Virtual, March 2022.<br>-->
               <a href="https://arxiv.org/abs/2406.03674">Preprint</a> /
                <a href="https://www.dropbox.com/scl/fi/rh3gn02lzhxegz37g2s53/multi_unit_auctions_RMP.pdf?rlkey=g5rreukqpemh7nnrflbgout7n&dl=0">Slides</a> /
               <a href="https://github.com/sourav22899/vm-buyers-up-auctions">Code</a> /
<!--               <a href="./files/experts.bib">BibTex</a>-->
                <p></p>
		</p><p></p>
                <p></p>
              </td>
            </tr>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>
<!--               <td width="25%"><img src="./img/n=2498_alpha=0.1.svg" alt="PontTuset" width="200" style="border-style: none">-->
	              </td><td width="75%" valign="top">
	        <a href="https://arxiv.org/abs/2209.14222">
                <papertitle>Online Subset Selection using &alpha;-Core with no Augmented Regret</papertitle></a>
               <br><span style="text-decoration: underline;">Sourav Sahoo</span>, Siddhant Chaudhary, Samrat Mukhopadhyay, and Abhishek Sinha<br>
<!--               International Conference on Artificial Intelligence and Statistics (AISTATS), Virtual, March 2022.<br>-->
               <a href="https://arxiv.org/abs/2209.14222">Preprint</a>
<!--               <a href="./files/experts.bib">BibTex</a>-->
                <p></p>
		</p><p></p>
                <p></p>
              </td>
            </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Conferences</heading>
              </td>
            </tr>
          </tbody></table>

                  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>
<!--               <td width="25%"><img src="./img/Regrets_T_q.png" alt="PontTuset" width="200" style="border-style: none">-->
	              </td><td width="75%" valign="top">
	        <a href="https://arxiv.org/abs/2109.12340">
                <papertitle>Distributed Online Optimization with Byzantine Adversarial Agents</papertitle></a>
               <br><span style="text-decoration: underline;">Sourav Sahoo</span>, Anand Gokhale and Rachel Kalpana Kalaimani<br>
               American Control Conference (ACC), Atlanta, United States, June 2022.<br>
               <a href="https://ieeexplore.ieee.org/document/9867506">Paper</a> /
               <a href="./files/dist_opt.bib">BibTex</a>
                <p></p>
		</p><p></p>
                <p></p>
              </td>
            </tr>


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>
<!--               <td width="25%"><img src="./img/n=2498_alpha=0.1.svg" alt="PontTuset" width="200" style="border-style: none">-->
	              </td><td width="75%" valign="top">
	        <a href="https://arxiv.org/abs/2110.07881">
                <papertitle>k-experts: Online Policies and Fundamental Limits</papertitle></a>
               <br>Samrat Mukhopadhyay, <span style="text-decoration: underline;">Sourav Sahoo</span>, and Abhishek Sinha<br>
               International Conference on Artificial Intelligence and Statistics (AISTATS), Virtual, March 2022.<br>
               <a href="https://proceedings.mlr.press/v151/mukhopadhyay22a.html">Paper</a> /
               <a href="https://github.com/sourav22899/k-sets-problem">Code</a> /
               <a href="./files/experts.bib">BibTex</a>
                <p></p>
		</p><p></p>
                <p></p>
              </td>
            </tr>
	  
	
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>
               <!--Next paper should be listed in bottom -->
<!--	              <td width="25%"><img src="./img/vis_rep.png" alt="PontTuset" width="200" style="border-style: none">-->
	              </td><td width="75%" valign="top">
	                <p><a href="https://link.springer.com/chapter/10.1007/978-3-030-41299-9_34">
                        <papertitle>A Segment Level Approach to Speech Emotion Recognition using Transfer Learning</papertitle></a>
                        <br><span style="text-decoration: underline;">Sourav Sahoo</span>, Puneet Kumar, Balasubramanian Raman and Partha Pratim Roy<br>
                        Asian Conference on Pattern Recognition (ACPR), Auckland, New Zealand, November 2019. <br>
                <a href="./files/segment_ser_paper64.pdf">Paper</a> /
                        <a href="./files/segment_ser_supplement_paper64.pdf">Supplementary</a> /
                        <a href="./files/acpr_paper_64_poster.pdf">Poster</a> /
                        <a href="https://github.com/sourav22899/segment-ser">Code</a> /
                        <a href="./files/acpr_paper.bib">BibTex</a>
               <p></p>
<!--               <p>In this paper, we propose a speech emotion recognition system that predicts emotions for multiple segments of a single audio clip unlike the conventional emotion recognition models that predict the emotion of an entire audio clip directly. The proposed system consists of a pre-trained deep convolutional neural network (CNN) followed by a single layered neural network which predicts the emotion classes of the audio segments. The predictions for the individual segments are finally combined to predict the emotion of a particular clip. The proposed model attains an accuracy of 68.7% surpassing the current state-of-the-art models in classifying the data into one of the four emotional classes <i>(angry, happy, sad and neutral)</i> when trained and evaluated on IEMOCAP audio-only dataset.-->

		</p><p></p><p></p>
              </td>
            </tr>
                     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Workshops</heading>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>
<!--               <td width="25%"><img src="./img/full_system1.png" alt="PontTuset" width="200" style="border-style: none">-->
	              </td><td width="75%" valign="top">
	        <a href="https://arxiv.org/abs/2012.00096">
                <papertitle>Multi-Modal Detection of Alzheimer's Disease from Speech and Text</papertitle></a>
               <br>Amish Mittal*, <span style="text-decoration: underline;">Sourav Sahoo*</span>, Arnhav Datar*, Juned Kadiwala*, Hrithwik Shalu and Jimson Mathew<br>
                International Workshop on Data Mining in Bioinformatics (BIOKDD), Virtual, August 2021.<br>
               <a href="https://arxiv.org/abs/2012.00096">ArXiv</a> /
               <a href="./files/alzheimers.bib">BibTex</a>
                <p></p>
<!--               <p>Reliable detection of the prodromal stages of Alzheimer's disease (AD) remains difficult even today because, unlike other neurocognitive impairments, there is no definitive diagnosis of AD <i>in vivo</i>. In this work, we propose a multimodal deep learning method that utilizes speech and the corresponding transcript simultaneously to detect AD. We also perform experiments to analyze the model performance when Automated Speech Recognition (ASR) system generated transcripts are used instead of manual transcription in the text-based model. The proposed method achieves 85.3% 10-fold cross-validation accuracy when trained and evaluated on the DementiaBank Pitt corpus.-->
                   <br><small>*Authors contributed equally</small><br>
		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
<!--	              <td width="25%"><img src="./img/vis_rep.png" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>Planar Geometry and Latest Scene Recovery from a Single Motion Blurred Image</papertitle></a><br>Kuldeep Purohit, Subeesh Vasu, M. Purnachandra Rao, and A.N. Rajagopalan<br>-->
<!--                  Under Review* <br>-->
<!--                  <a href="https://arxiv.org/abs/1904.03710">ArXiv Version</a> -->
<!--           &lt;!&ndash;       <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p>Existing works on motion deblurring either ignore the effects of depth-dependent blur or work with the assumption of a -->
<!--			multi-layered scene wherein each layer is modeled in the form of fronto-parallel plane. In this work, we consider-->
<!--			the case of 3D scenes with piecewise planar structure i.e., a scene that can be modeled as a combination of multiple -->
<!--			planes with arbitrary orientations. We first propose an approach for estimation of normal of a planar scene from a -->
<!--			single motion blurred observation. We then develop an algorithm for automatic recovery of a number of planes, the -->
<!--			parameters corresponding to each plane, and camera motion from a single motion blurred image of a multiplanar 3D scene.-->
<!--			Finally, we propose a first-of-its-kind approach to recover the planar geometry and latent image of the scene by -->
<!--			adopting an alternating minimization framework built on our findings. Experiments on synthetic and real data reveal -->
<!--			that our proposed method achieves state-of-the-art results.-->
<!--                    &lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Other Projects</heading>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody><tr>

<!--                          <td width="25%"><img src="./img/fastlr.png" alt="PontTuset" width="200" style="border-style: none">-->
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>SVRG-SO: SVRG for Stochastic Optimization</papertitle></a>
                        <br>Sourav Sahoo<br>
                        CS6515 Final Project<br>
                        <a href="./files/cs6515_project.pdf">Report</a>
                </p><p></p>
<!--                <p> Stochastic Variance Reduced Gradient (SVRG) belongs to a family of gradient aggregation algorithms widely-->
<!--                    used in large-scale machine learning applications. SVRG achieved linear convergence for strongly convex finite- -->
<!--                    sum optimization problems. Although similar results were also obtained by previous algorithms such as stochastic-->
<!--                    dual coordinate ascent (SDCA) and stochastic average gradient (SAG), SVRG is more intuitive and simpler-->
<!--                    to analyse. Furthermore, SVRG also works well in practice for non-strongly convex or non-convex finite-sum-->
<!--                    problems. Owing to such robustness and simplicity of the algorithm, we adapt it for stochastic optimization-->
<!--                    problems in this work. In this pursuit, we present SVRG for Stochastic Optimization (SVRG-SO) and recover-->
<!--                    optimal convergence rates for strongly convex and smooth objective functions.</p>-->
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--	              <td width="25%"><img src="./img/2_params.png" alt="PontTuset" width="200" style="border-style: none">-->
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Convergence and Implicit Regularization of Stochastic Mirror Descent in Overparameterized Models</papertitle></a>
                        <br>Sourav Sahoo<br>
                        EE5121 Term Paper<br>
                        <a href="./files/ee_5121_term_paper.pdf">Report</a> /
                        <a href="https://github.com/sourav22899/ee5121-convex/tree/master/term-paper">Code</a>
                </p><p></p>
<!--                <p> Stochastic Mirror Descent (SMD) algorithms comprise a family of algorithms that are increasingly being used in multiple domains such as machine learning, optimization, signal processing, and control. Like Stochastic Gradient Descent (SGD), SMD algorithms perform updates along the negative gradient of a stochastically chosen loss function. However, instead of performing updates directly on the objective parameter, updates are performed in a "mirror domain" whose transformation is given by the gradient of a strictly convex function. In this work, we explicitly shed some light on <i>convergence</i> and <i>implicit regularization</i> due to stochastic mirror descent in overparameterized linear and non-linear models.-->
                </p><p></p>
                <p></p>
              </td>
            </tr>




<!--	              <td width="25%"><img src="./files/segment_ser_supplement_paper64.pdf" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>Multiple Target Detection and Tracking in Wide Area Surveillance</papertitle></a><br>Kuldeep Purohit and Arshad Jamal (Scientist E)<br>-->
<!--                  Project under Centre for Artificial Intelligence and Robotics, Defense Research and Development Organization, India &nbsp; <br>-->
<!--                  2012 &nbsp; <br>-->
<!--                 <a href="./files/segment_ser_supplement_paper64.pdf">Report</a>-->
<!--	&lt;!&ndash; 	<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">supplementary</a> / &ndash;&gt;-->
<!--           &lt;!&ndash;       <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a>  &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> In this project, the problem of object detection and tracking in the challenging domain of wide area surveillance-->
<!--			has been tackled. This problem poses several challenges: large camera motion, strong parallax, large number of moving-->
<!--			objects, and small number of pixels on target, single channel data and low frame-rate of video. The method implemented-->
<!--			here overcomes these challenges when tested on UAV videos.-->
<!--</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--Next paper should be listed in bottom -->
<!--

	              <td width="25%"><img src="./img/PhDthesis.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://arxiv.org/abs/1512.07314">
	<papertitle>Mid-level Representation for Visual Recognition</papertitle></a><br>
                  Moin Nabi<br>
                  <em>Ph.D. Dissertation </em>, 2015 &nbsp; <br>
                  <a href="./files/PhD_thesis.pdf">PDF</a> /
                  <a href="./files/thesis_slide.pdf">slides</a> /
                  <a href="https://www.youtube.com/watch?v=6IY-0swKaiM">talk</a> /
                </p><p></p>
                <p>This thesis targets employing mid-level representations for different high-level visual
recognition tasks, both in image and video understanding.
                </p><p></p>
                <p></p>
              </td>
            </tr>

-->
	            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
<div style="text-align: left;"><script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5fz2vvb1pjx&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
              </td>
            </tr>
          </tbody></table>



	<!-- <script type="text/javascript" src="//ri.revolvermaps.com/0/0/1.js?i=834fq7qvtyr&amp;s=182&amp;m=0&amp;v=false&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000"align="left" async="async"></script></div>
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5fz2vvb1pjx&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script> -->
</body></html>
