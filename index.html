<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- original template from from url=(0035)http://www.cs.berkeley.edu/~barron/ -->
<html lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
     <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
	
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link rel="icon" type="image/png" href="http://www.cs.berkeley.edu/~barron/seal_icon.png">
	
    <title>Sourav Sahoo</title>
    
    <link href="/img/css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
      <tbody><tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center">
                  <name>Sourav Sahoo</name>
                  
                </p><p align="">I am a fourth-year student in the dual degree program at the <a href="http://www.ee.iitm.ac.in/"> Department of Electrical Engineering at Indian Institute of Technology Madras, Chennai</a>. My research lies at the intersection of Image Processing/Computer Vision and Deep Learning. My primary research interests are machine learning theory, deep learning, and optimization. Previously, I have worked on multiple projects involving the application of machine learning in different domains.

                </p><p align="center">
			
<a href="mailto:sourav.sahoo@smail.iitm.ac.in">Email</a> &nbsp;/&nbsp;
<a href="https://scholar.google.co.in/citations?hl=en&authuser=1&user=IWZJF8wAAAAJ">Google Scholar</a> /&nbsp;
<a href="https://www.researchgate.net/profile/Sourav_Sahoo4">ResearchGate</a> /&nbsp;
<a href="https://www.linkedin.com/in/sourav-sahoo-68a07615a/"> LinkedIn </a> /&nbsp;
<a href="https://twitter.com/sourav22899"> Twitter </a> /&nbsp;
<a href="https://github.com/sourav22899"> GitHub </a> /&nbsp;
<a href="./files/cv_updated.pdf"> CV </a>
                </p>
              </td>
			  <td width="33%">
			  <img src="./img/20190510_111651.jpg" alt="" width="100%" align="top">
			  </td>
<!-- 				<td> <img src="./img/20190510_111651.jpg" style="width: 200;"></td></tr>  -->
          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>News</heading>
		<p> [2020.06] Joined as a Post-doctoral Research Associate at the <a href="http://www.cse.msu.edu/">Computer Science and Engineering Department at Michigan State University</a> under <a href="http://hal.cse.msu.edu/team/vishnu-boddeti/">Prof. Vishnu Boddeti</a>.</p>
		<p> [2020.03] Our <a href="https://arxiv.org/abs/2004.05343">paper</a> has been accepted at <strong><a href="http://cvpr2020.thecvf.com/">CVPR 2020</a></strong>, Seattle, USA.</p>
		<p> [2019.12] Invited to be a reviewer for the IEEE Transactions on Image processing (TIP), IEEE Transactions on Multimedia (TMM) and International Journal of Computer Vision (IJCV) (Springer). </p>

              </td>
            </tr>
          </tbody></table>

	  
	  
	  
	  <!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Publications and Preprints</heading>
              </td>
            </tr>
          </tbody></table>

	  
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>	  

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/vis_rep.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/1605.07651">
                        <papertitle>Depth-guided Dense Dynamic Filtering Network for Bokeh Effect Rendering</papertitle></a>
                        <br><strong>Kuldeep Purohit</strong>*, Maitreya Suin, Praveen Kandula, and A.N. Rajagopalan<br>
                        <strong>AIM Workshop and Challenge, International Conference on Computer Vision (ICCV 2019), Seoul, South Korea,
                            November 2019</strong> <br>
                <p></p>
                <p>Our work presented at the International Conference on Computer Vision (ICCV) - Advances in Image Manipulation (AIM) Workshop 2019. Our team was a <strong>runner-up in both the tracks of Bokeh Effect Challenge</strong> (http://www.vision.ee.ethz.ch/aim19/).
<!-- <small>*Authors contributed equally</small>-->

		</p><p></p>
                <p></p>
              </td>
            </tr>	  
	  
	
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>
               <!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/vis_rep.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="https://arxiv.org/abs/1605.07651">
                        <papertitle>Bringing Alive Blurred Moments</papertitle></a>
                        <br><strong>Kuldeep Purohit</strong>*, Anshul Shah, and A.N. Rajagopalan<br>
                        <strong>Accepted for Oral Presentation at IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019), Long Beach, CA, USA, June 2019</strong> <br>
                <a href="./files/segment_ser_paper64.pdf">Paper</a> /
                        <a href="./files/segment_ser_supplement_paper64.pdf">Supplementary</a> /
                        <a href="./files/acpr_paper_64_poster.pdf">Poster</a> /
                        <a href="https://github.com/sourav22899/segment-ser">Code</a> /
                        <a href="./files/acpr_paper.bib">BibTex</a> -->
               <p></p>
                <p>Designed a deep convolutional architecture to extract a sharp video from a motion blurred
                image. The first stage involves unsupervised training of a novel spatiotemporal network
                for motion extraction from short video sequences. The above network is utilized for guided
                training of a CNN which extracts the same motion embedding from a single blurred image.
                The above networks are finally linked with our efficient deblurring network to generate the
                sharp video. Our framework delivers state-of-the-art accuracy in single image deblurring
                and video extraction while being faster and more compact.
                    <!-- <small>*Authors contributed equally</small>-->
		</p><p></p>
                <p></p>
              </td>
            </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/vis_rep.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	        <a href="https://arxiv.org/abs/1605.07651"> -->
	<papertitle>Mixed-Dense Connection Networks for Image and Video Super-Resolution</papertitle></a><br><strong>Kuldeep Purohit</strong>*, Srimanta Mandal, and A.N. Rajagopalan<br>
			                  <strong>Elsevier Neurocomputing (Special Issue on Deep Learning for Image Super-Resolution) 2019</strong> <br>
                <a href="https://www.sciencedirect.com/science/article/pii/S0925231219314614?via%3Dihub">Paper Link</a>
	 <!--  	  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a> /
                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/SanginetoNCS16.bib">bibtex</a> -->
                <p></p>
                <p>Proposed a deep architecture for image and video super-resolution, which is built using efficient convolutional units we refer to as mixed-dense connection blocks, whose design combines the strengths of both residual and dense connection strategies, while overcoming their limitations. We enable efficient super-resolution for higher scale-factors through our scale-recurrent framework which reutilizes the filters learnt for lower scale factors recursively for higher factors. We analyze the effects of loss configurations and demonstrate their utility in enhancing complementary image qualities. The proposed networks lead to state-of-the-art results on image and video super-resolution benchmarks.
<!-- <small>*Authors contributed equally</small>-->
		</p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/vis_rep.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Planar Geometry and Latest Scene Recovery from a Single Motion Blurred Image</papertitle></a><br><strong>Kuldeep Purohit</strong>, Subeesh Vasu, M. Purnachandra Rao, and A.N. Rajagopalan<br>
                  <strong>Under Review</strong>* <br>
                  <a href="https://arxiv.org/abs/1904.03710">ArXiv Version</a> 
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p>Existing works on motion deblurring either ignore the effects of depth-dependent blur or work with the assumption of a 
			multi-layered scene wherein each layer is modeled in the form of fronto-parallel plane. In this work, we consider
			the case of 3D scenes with piecewise planar structure i.e., a scene that can be modeled as a combination of multiple 
			planes with arbitrary orientations. We first propose an approach for estimation of normal of a planar scene from a 
			single motion blurred observation. We then develop an algorithm for automatic recovery of a number of planes, the 
			parameters corresponding to each plane, and camera motion from a single motion blurred image of a multiplanar 3D scene.
			Finally, we propose a first-of-its-kind approach to recover the planar geometry and latent image of the scene by 
			adopting an alternating minimization framework built on our findings. Experiments on synthetic and real data reveal 
			that our proposed method achieves state-of-the-art results.
                    <!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>


<!--           <td width="25%"><img src="./img/UW_Dehazing.png" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>Multi-level Weighted Enhancment for Underwater Image Dehazing</papertitle></a><br><strong>Kuldeep Purohit</strong>, Srimanta Mandal and A.N. Rajagopalan<br>-->
<!--                  <strong>Journal of the Optical Society of America A (JOSA-A) </strong>* <br>-->
<!--                  <a href="https://www.osapublishing.org/josaa/abstract.cfm?uri=josaa-36-6-1098">Paper Link</a> -->
<!--           &lt;!&ndash;       <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p>Attenuation and scattering of light are responsible for haziness in images of underwater-->
<!--scenes. We propose an approach to reduce this effect, based on the underlying principle is that enhancement at different-->
<!--levels of detail can undo the degradation caused by underwater haze. The depth information is-->
<!--captured implicitly while going through different levels of details due to depth-variant nature of-->
<!--haze. Hence, we judiciously assign weights to different levels of image details and reveal that-->
<!--their linear combination along with the coarsest information can successfully restore the image.-->
<!--Results demonstrate the efficacy of our approach as compared to state-of-the-art underwater-->
<!--dehazing methods. -->
<!--</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--&lt;!&ndash;Next paper should be listed in bottom &ndash;&gt;-->
<!--	              <td width="25%"><img src="./files/ICCV_thumbprint.png" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>EFFICIENT MOTION DEBLURRING WITH FEATURE TRANSFORMATION AND SPATIAL ATTENTION</papertitle></a><br><strong>Kuldeep Purohit</strong> and A.N. Rajagopalan<br>-->
<!--                  <strong>Accepted at the IEEE International Conference on Image Processing (ICIP) 2019</strong>* <br>-->
<!--                  <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> -->
<!--           &lt;!&ndash;       <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> A preliminary version of our work "Spatially-Adaptive Residual Networks for Efficient Image and Video Deblurring"</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--		<tbody>-->
<!--		   <tr>-->


<!--&lt;!&ndash;Next paper should be listed in bottom &ndash;&gt;-->
<!--	              <td width="25%"><img src="./img/PD_plot.png" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	        &lt;!&ndash;        <p><a href="https://arxiv.org/abs/1605.07651"> &ndash;&gt;-->
<!--	<papertitle>Scale-Recurrent Multi-residual Dense Network for Image Super-Resolution</papertitle></a><br><strong>Kuldeep Purohit</strong>*, Srimanta Mandal, and A.N. Rajagopalan<br>-->
<!--			                  <strong>PIRM Workshop and Challenge, Eurpean Conference on Computer Vision Workshops (ECCVW 2018), Munich, Germany, September 2018</strong> <br>-->

<!-- <a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Purohit_Scale-Recurrent_Multi-Residual_Dense_Network_for_Image_Super-Resolution_ECCVW_2018_paper.pdf">Paper</a> /-->
<!-- <a href="./files/Poster_ECCV2018.pdf">Poster</a> -->

<!--          &lt;!&ndash;       <a href="https://arxiv.org/abs/1804.02913.pdf">ArXiv Pre-print</a>-->
<!--	  	  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a> /-->
<!--                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/SanginetoNCS16.bib">bibtex</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p>A preliminary version of our Neurocomputing work, presented at the European Conference on Computer Vision (ECCV) - Perceptual Image Restoration and Manipulation (PIRM) Workshop 2018. Our team REC-SR was a <strong>finalist in all three regions of the Super Resolution Challenge</strong> (https://www.pirm2018.org/PIRM-SR.html).-->
<!--</br></br> -->
<!--&lt;!&ndash; <small>*Authors contributed equally</small>&ndash;&gt;-->

<!--		</p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->




<!--&lt;!&ndash;Next paper should be listed in bottom &ndash;&gt;-->
<!--	              <td width="25%"><img src="./img/blur_detection.jpg" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	   &lt;!&ndash;              <p><a href="https://arxiv.org/abs/1610.00307"> &ndash;&gt;-->
<!--	<papertitle>Learning based Blur Detection and Segmentation</papertitle></a><br>-->
<!--		 <strong>Kuldeep Purohit</strong>, Anshul B. Shah, and A.N. Rajagopalan<br>-->
<!--                  <strong>IEEE International Conference on Image Processing (ICIP 2018), Athens, Greece, October 2018  </strong> <br>-->
<!--                  <a href="https://ieeexplore.ieee.org/document/8451765">Paper Link</a> /-->
<!--                  <a href="./files/ICIP2018_supplementary.pdf">Supplementary</a> /-->
<!--                  <a href="./files/POSTER_ICIP2018.pdf">Poster</a> -->
<!--        &lt;!&ndash;           <em>arXiv:1610.00307</em>, 2016 &nbsp; <br>-->
<!--                  <a href="https://arxiv.org/pdf/1610.00307v1.pdf">PDF</a> /-->
<!--                  <a href="#">bibtex</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p>We present a robust two-level architecture for blur-based segmentation of a single image. First network is a fully convolutional encoder-decoder for estimating a semantically meaningful blur map from the full-resolution blurred image. Second network is a CNN-based classifier for obtaining local (patch-level) blur-probabilities. Fusion of the two network outputs enables accurate blur-segmentation using Graph-cut optimization over the obtained probabilities. We also show its applications in blur magnification and matting.-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->


<!--&lt;!&ndash;Next paper should be listed in bottom &ndash;&gt;-->
<!--	              <td width="25%"><img src="./img/icvgip2019.png" alt="PontTuset" width="150" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	   &lt;!&ndash;              <p><a href="https://arxiv.org/abs/1610.00307"> &ndash;&gt;-->
<!--	<papertitle>Color Image Super Resolution in Real Noise</papertitle></a><br>-->
<!--		 Srimanta Mandal, <strong>Kuldeep Purohit</strong>, and A.N. Rajagopalan<br>-->
<!--                  <strong>Accepted for Oral Presentation at ACM Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP 2018), IIIT Hyderabad, India, december 2018 </strong> <br>-->
<!--                   <a href="./files/ICVGIP2018_Cam.pdf">Accepted Version</a> -->
<!--         &lt;!&ndash;         <em>arXiv:1610.00307</em>, 2016 &nbsp; <br>-->
<!--                  <a href="https://arxiv.org/pdf/1610.00307v1.pdf">PDF</a> /-->
<!--                  <a href="#">bibtex</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> Proposed an approach to super-resolve noisy color images by considering the color channels-->
<!--jointly. Implicit low-rank structure of visual data is enforced via nuclear norm minimization-->
<!--in association with color channel-dependent weights, which are added as a regularization-->
<!--term to the cost function. Additionally, multi-scale details of the image are added to the-->
<!--model through another regularization term that involves projection onto PCA basis, which-->
<!--is constructed using similar patches extracted across different scales of the input image. <strong> Selected for the Best Paper Award (Runner Up) </strong>*: https://cvit.iiit.ac.in/icvgip18/bestpaperaward.php.-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->


<!--&lt;!&ndash;Next paper should be listed in bottom &ndash;&gt;-->
<!--	              <td width="25%"><img src="./img/ICVGIP.png" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>Mosaicing Deep Underwater Imagery</papertitle></a><br><strong>Kuldeep Purohit</strong>,Subeesh Vasu, A.N. Rajagopalan, V Bala Naga Jyothi, and Ramesh Raju<br>-->
<!--                  <strong>ACM Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP 2016), IIT Guwahati, India, december 2016 </strong> &nbsp; <br>-->
<!--                  <a href="http://www.ee.iitm.ac.in/~ee13d050/pdf/2016_Kuldeep_Deep_icvgip.pdf">main paper</a> /-->
<!--		<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">Supplementary</a> /-->
<!--		<a href="./files/POSTER_ICVGIP2016.pdf">Poster</a> -->
<!--           &lt;!&ndash;       <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a>  &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p>This work deals with the problem of mosaicing deep underwater images (captured by Remotely Operated Vehicles), which suffer from haze, color-cast, and non-uniform illumination. We propose a framework that restores these images in accordance with a suitably derived degradation model. Furthermore, our scheme harnesses the scene-depth information present in the haze for non-rigid registration of the images before blending to construct a mosaic that is free from artifacts such as local blurring, ghosting, and visible seams.-->
<!--</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--&lt;!&ndash;Next paper should be listed in bottom &ndash;&gt;-->
<!--	              <td width="25%"><img src="./img/splicing.png" alt="PontTuset" width="170" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>Splicing Localization in Motion Blurred 3D scenes</papertitle></a><br><strong>Kuldeep Purohit</strong> and A.N. Rajagopalan<br>-->
<!--                  <strong>IEEE International Conference on Image Processing (ICIP 2016), Phoenix, Arizona, September 2016</strong> &nbsp; <br>-->
<!--                  <a href="https://ieeexplore.ieee.org/abstract/document/7533095">Paper</a> /				-->
<!--                  <a href="./files/ICIP2016_supplementary.pdf">Supplementary</a> /-->
<!--                  <a href="./files/POSTER_ICIP2016.pdf">Poster</a> -->
<!--      &lt;!&ndash;            <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> This work proposes an efficient algorithm for depth based segmentation using spatially-distributed blur-kernels present in a single motion-blurred image of a 3D scene. The segmentation is then further utilized to estimate global camera motion from a single blurred image of a 3D scene. Finally, local blur profiles are compared with the global motion model to highlight inconsistencies and detect spliced regions. -->
<!--</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--            <tbody><tr>-->
<!--              <td>-->
<!--                <heading>Co-authored Workshop Proceedings</heading>-->
<!--              </td>-->
<!--            </tr>-->
<!--          </tbody></table>-->
<!--           -->
<!--          <table width="100%" align="center" border="0" cellpadding="20">-->
<!--            <tbody><tr>-->

<!--	              <td width="25%"><img src="./files/NTIRE2019.jpeg" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>NTIRE 2019 Challenge on Image Colorization: Report</papertitle></a><br>-->
<!--Shuhang Gu, Radu Timofte, Richard Zhang, Maitreya Suin, <strong>Kuldeep Purohit</strong> , A. N. Rajagopalan, Athi Narayanan S., Jameer Babu Pinjari, Zhiwei Xiong, Zhan Shi, Chang Chen, Dong Liu, Manoj Sharma, Megh Makwana, Anuj Badhwar, Ajay Pratap Singh, Avinash Upadhyay, Akkshita Trivedi, Anil Saini, Santanu Chaudhury, Prasen Kumar Sharma, Priyankar Jain, Arijit Sur, Gokhan Ozbulak <br>-->
<!--                  <strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</strong> <br>-->
<!--                  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Gu_NTIRE_2019_Challenge_on_Image_Colorization_Report_CVPRW_2019_paper.pdf">PDF (Openacess)</a> -->
<!--          &lt;!&ndash;        <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> Report describing various solutions to the Challenge on Image Colorization, NTIRE 2019</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--		<tbody>-->
<!--		   <tr>		   -->
<!--			   <td width="25%"><img src="./files/NTIRE2019.jpeg" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>NTIRE 2019 Challenge on Video Deblurring: Methods and Results</papertitle></a><br>-->
<!--Nah, Seungjun and Timofte, Radu and Baik, Sungyong and Hong, Seokil and Moon, Gyeongsik and Son, Sanghyun and Lee, Kyoung Mu and Wang, Xintao and Chan, Kelvin C.K. and Yu, Ke and Dong, Chao and Loy, Chen Change and Fan, Yuchen and Yu, Jiahui and Liu, Ding and Huang, Thomas S. and Sim, Hyeonjun and Kim, Munchurl and Park, Dongwon and Kim, Jisoo and Chun, Se Young and Haris, Muhammad and Shakhnarovich, Greg and Ukita, Norimichi and Zamir, Syed Waqas and Arora, Aditya and Khan, Salman and Khan, Fahad Shahbaz and Shao, Ling and Gupta, Rahul Kumar and Chudasama, Vishal and Patel, Heena and Upla, Kishor and Fan, Hongfei and Li, Guo and Zhang, Yumei and Li, Xiang and Zhang, Wenjie and He, Qingwen and <strong>Purohit, Kuldeep</strong> and Rajagopalan, A. N. and Kim, Jeonghun and Tofighi, Mohammad and Guo, Tiantong and Monga, Vishal <br>-->
<!--                  <strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</strong> <br>-->
<!--                  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Deblurring_Methods_and_Results_CVPRW_2019_paper.pdf">PDF (Openacess)</a> -->
<!--          &lt;!&ndash;        <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> -->
<!--                  <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> Report describing various solutions to the Challenge on Video Deblurring, NTIRE 2019</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--		<tbody>-->
<!--		   <tr>-->
<!--			   <td width="25%"><img src="./files/NTIRE2019.jpeg" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>NTIRE 2019 Challenge on Video Super-Resolution: Methods and Results</papertitle></a><br>-->
<!--Nah, Seungjun and Timofte, Radu and Gu, Shuhang and Baik, Sungyong and Hong, Seokil and Moon, Gyeongsik and Son, Sanghyun and Lee, Kyoung Mu and Wang, Xintao and Chan, Kelvin C.K. and Yu, Ke and Dong, Chao and Loy, Chen Change and Fan, Yuchen and Yu, Jiahui and Liu, Ding and Huang, Thomas S. and Liu, Xiao and Li, Chao and He, Dongliang and Ding, Yukang and Wen, Shilei and Porikli, Fatih and Kalarot, Ratheesh and Haris, Muhammad and Shakhnarovich, Greg and Ukita, Norimichi and Yi, Peng and Wang, Zhongyuan and Jiang, Kui and Jiang, Junjun and Ma, Jiayi and Dong, Hang and Zhang, Xinyi and Hu, Zhe and Kim, Kwanyoung and Kang, Dong Un and Chun, Se Young and <strong>Purohit, Kuldeep</strong> and Rajagopalan, A. N. and Tian, Yapeng  and Zhang, Yulun and Fu, Yun and Xu, Chenliang and Tekalp, A. Murat and Yilmaz, M. Akin and Korkmaz, Cansu and Sharma, Manoj and Makwana, Megh and Badhwar, Anuj and Singh, Ajay Pratap and Upadhyay, Avinash and Mukhopadhyay, Rudrabha and Shukla, Ankit and Khanna, Dheeraj and Mandal, A.S. and Chaudhury, Santanu and Miao, Si and Zhu, Yongxin and Huo, Xiao <br>-->
<!--                  <strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</strong> <br>-->
<!--                  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Super-Resolution_Methods_and_Results_CVPRW_2019_paper.pdf">PDF (Openacess)</a> -->
<!--          &lt;!&ndash;        <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> -->
<!--                  <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> Report describing various solutions to the Challenge on Video Super-Resolution, NTIRE 2019</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--		<tbody>-->
<!--		   <tr>-->
<!--               <td width="25%"><img src="./files/NTIRE2019.jpeg" alt="PontTuset" width="200" style="border-style: none">-->
<!--	              </td><td width="75%" valign="top">-->
<!--	                <p><a href="#">-->
<!--	<papertitle>NTIRE 2019 Image Dehazing Challenge Report</papertitle></a><br>-->
<!--Codruta O. Ancuti, Cosmin Ancuti, Radu Timofte, Luc Van Gool, Lei Zhang,Ming-Hsuan Yang, Tiantong Guo, Xuelu Li, Venkateswararao Cherukuri, Vishal Monga, Hao Jiang, Siyuan Yang, Yan Liu, Xiaochao Qu, Pengfei Wan, Dongwon Park, Se Young Chun, Ming Hong, Jinying Huang, Yizi Chen, Shuxin Chen, Bomin Wang, Pablo Navarrete Michelini, Hanwen Liu, Dan Zhu, Jing Liu, Sanchayan Santra, Ranjan Mondal , Bhabatosh Chanda, Peter Morales, Tzofi Klinghoffer, Le Manh Quan, Yong-Guk Kim, Xiao Liang, Runde Li, Jinshan Pan, Jinhui Tang, <strong>Kuldeep Purohit</strong> , Maitreya Suin, Raimondo Schettini, Simone Bianco, Flavio Piccoli, C. Cusano, Luigi Celona, Sunhee Hwang, Hyeran Byun, Subrahmanyam Murala, Akshay Dudhane, Harsh Aulakh, Zheng Tianxiang, Tao Zhang, Weining Qin, Runnan Zhou, Shanhu Wang, Jean-Philippe Tarel, Chuansheng Wang, Jiawei Wu <br>-->
<!--                  <strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</strong> <br>-->
<!--                  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Ancuti_NTIRE_2019_Image_Dehazing_Challenge_Report_CVPRW_2019_paper.pdf">PDF (Openacess)</a> -->
<!--          &lt;!&ndash;        <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> -->
<!--                  <a href="./files/UW2014_slides.pdf">slide</a> /-->
<!--                  <a href="#">code</a> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p> Report describing various solutions to the Challenge on Image Dehazing, NTIRE 2019</br></br>-->
<!--&lt;!&ndash; <small>*This work was done while I was at UW.</small> &ndash;&gt;-->
<!--                </p><p></p>-->
<!--                <p></p>-->
<!--              </td>-->
<!--            </tr>-->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--		<tbody>-->
<!--		   <tr>-->


<!--SECTION -->




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Other Projects</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody><tr>




	              <td width="25%"><img src="./img/vis_rep.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Enhancement and Photo-metric Stereo for SEM images</papertitle></a><br><strong>Kuldeep Purohit</strong>, Arun M, Mahesh Mohan, and A.N. Rajagopalan<br>
                  <strong>Project with KLA-Tencor, India</strong> &nbsp; <br>
                  2017 &nbsp; <br>
              <!--    <a href="http://www.ee.iitm.ac.in/~ee13d050/pdf/2016_Kuldeep_Deep_icvgip.pdf">main paper</a> /
		<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">supplementary</a> / -->
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>  -->
                </p><p></p>
                <p> Addressed the blind reconstruction problem in scanning electron microscope (SEM) photometric stereo for complicated semiconductor patterns to be measured.
			Developed a scheme using domain-specific priors on surface and sensor patterns in the optimization framework for robust estimation of the 3D surface structures.
			Also developed a user-centric detail enhancement scheme for improving visual quality of noisy SEM images.
			Proposed appraoch was validated through experiments on real data and was deployed commercially.
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>


	              <td width="25%"><img src="./files/segment_ser_supplement_paper64.pdf" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Autonomous Bot for the Annual Intelligent Ground Vehicle Competition (IGVC)</papertitle></a>
                  <strong>Team Abhiyaan, Centre for Innovation (CFI), IIT Madras, India</strong> &nbsp; <br>
                  2017 &nbsp; <br>
                 <a href="http://www.igvc.org/design/2017/7.pdf">Report</a>
	<!-- 	<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">supplementary</a> / -->
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>  -->
                </p><p></p>
                <p> Our team designed a fully autonomous all-terrain ground vehicle. I specifically worked on the Computer Vision Module which involved algorithm development for
	        real-time lane detection and obstacle segmentation task. The computer vision and path planning modules were integrated using ROS for autonomous navigation.
		The design was used in Vehicles that represented IIT Madras in the Intelligent Ground Vehicle Competition (IGVC) in 2017 and 2018.
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

	              <td width="25%"><img src="./files/segment_ser_supplement_paper64.pdf" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Multiple Target Detection and Tracking in Wide Area Surveillance</papertitle></a><br><strong>Kuldeep Purohit</strong> and Arshad Jamal (Scientist E)<br>
                  <strong>Project under Centre for Artificial Intelligence and Robotics, Defense Research and Development Organization, India</strong> &nbsp; <br>
                  2012 &nbsp; <br>
                 <a href="./files/segment_ser_supplement_paper64.pdf">Report</a>
	<!-- 	<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">supplementary</a> / -->
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>  -->
                </p><p></p>
                <p> In this project, the problem of object detection and tracking in the challenging domain of wide area surveillance
			has been tackled. This problem poses several challenges: large camera motion, strong parallax, large number of moving
			objects, and small number of pixels on target, single channel data and low frame-rate of video. The method implemented
			here overcomes these challenges when tested on UAV videos.
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
<!--

	              <td width="25%"><img src="./img/PhDthesis.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://arxiv.org/abs/1512.07314">
	<papertitle>Mid-level Representation for Visual Recognition</papertitle></a><br>
                  <strong>Moin Nabi</strong><br>
                  <em>Ph.D. Dissertation </em>, 2015 &nbsp; <br>
                  <a href="./files/PhD_thesis.pdf">PDF</a> /
                  <a href="./files/thesis_slide.pdf">slides</a> /
                  <a href="https://www.youtube.com/watch?v=6IY-0swKaiM">talk</a> /
                </p><p></p>
                <p>This thesis targets employing mid-level representations for different high-level visual
recognition tasks, both in image and video understanding.
                </p><p></p>
                <p></p>
              </td>
            </tr>

-->

<!--Next paper should be listed in bottom -->

<!--
	              <td width="25%"><img src="./img/stock.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Stock trend prediction using Twin Gaussian Process regression</papertitle></a><br>M. Mojaddady, <strong>M. Nabi</strong>, S. Khadivi<br>
                  <em>Technical Report</em>, 2011 &nbsp;<br>
                  <a href="./files/stock.pdf">PDF</a> /
                  <a href="./files/stock.bib">bibtex</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>

-->

<!--Next paper should be listed in bottom -->

<!--

	              <td width="25%"><img src="./img/matlab3.jpg" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>A Turorial on Digital Image Processing using MATLAB</papertitle></a><br><strong>M. Nabi</strong><br>
                  <em>National Digital Image Processing Workshop</em>, 2008 &nbsp;<br>
                  <a href="./files/tutorial.zip">PDF</a> /
		  <a href="./files/code.zip">code</a> /
		  <a href="./files/images.tar.gz">data</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>

-->

<!--                <br>
                <p align="right"><font size="3">
			Erd&ouml;s = 3 (via two paths)
                  </font>
                <br>
-->


 <!-- 		<p align="right"><font size="2">
                  <a href="http://www.cs.berkeley.edu/~barron/">Thanks Jon Barron</a>
                  </font>
                </p>
              </td>
            </tr>
          </tbody></table>
          <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
                    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

          </script><script src="./img/ga.js" type="text/javascript"></script> <script type="text/javascript">
            try {
                    var pageTracker = _gat._getTracker("UA-7580334-1");
                    pageTracker._trackPageview();
                    } catch(err) {}

          </script>
        </td>
      </tr>
    </tbody></table>
 -->

	            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
<div style="text-align: left;"><script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5fz2vvb1pjx&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
              </td>
            </tr>
          </tbody></table>



	<!-- <script type="text/javascript" src="//ri.revolvermaps.com/0/0/1.js?i=834fq7qvtyr&amp;s=182&amp;m=0&amp;v=false&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000"align="left" async="async"></script></div>
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5fz2vvb1pjx&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script> -->
</body></html>
